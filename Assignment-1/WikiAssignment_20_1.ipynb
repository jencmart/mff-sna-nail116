{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment: Analyzing Wikipedia Pages\n",
    "\n",
    "In this assignment, you will analyze a small fraction of [Wikipedia](https://en.wikipedia.org/wiki/Main_Page) pages. For manipulating Wikipedia pages, you will use `wikipedia` module that could require an installation. You can install the module in several ways, it depends on your installation of python and Jupyter. If you have administrator rights, you can use\n",
    "``` \n",
    "conda install -c conda-forge wikipedia\n",
    "```\n",
    "when your installation used `conda` or \n",
    "```\n",
    "pip install wikipedia\n",
    "```\n",
    "that should work in any case. However, if you need to install it on a computer where you do not have administrator rights, you should use\n",
    "```\n",
    "pip install --user wikipedia\n",
    "```\n",
    "which installs the module in a subdirectory of your home directory. \n",
    "\n",
    "Official documentation for the `wikipedia` module can be found [here](https://wikipedia.readthedocs.io/en/latest/code.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import wikipedia\n",
    "\n",
    "wikipedia.set_lang(\"en\")    # we will search English version of Wikipedia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Module `wikipedia` enables us to load Wikipedia pages and extract links contained within the pages. E.g., let us download the page about the beatle [Bembidion-Ambiguum](https://en.wikipedia.org/wiki/Bembidion_ambiguum) and list all links from the page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "page = \"Bembidion-ambiguum\"\n",
    "\n",
    "try:\n",
    "    wiki = wikipedia.page(page)\n",
    "except wikipedia.DisambiguationError:\n",
    "    print(\"Page title\", page,\"is ambiguous\")\n",
    "except wikipedia.PageError:\n",
    "    print(\"No matching page for '\",page,\"' found\")\n",
    "    try:\n",
    "        wiki = wikipedia.page(page,auto_suggest=False)\n",
    "    except:\n",
    "        print(\"No matching page for '\",page,\"' found even with auto_suggest=False\")\n",
    "    print(\"auto_suggest=False helped!\")\n",
    "except:\n",
    "    print(\"Could not load page\",page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Wikipedia page on 'Bembidion-ambiguum' has the title 'Bembidion ambiguum' and contains 20 links\n",
      "Animal | Animal\n",
      "Arthropod | Arthropod\n",
      "Beetle | Beetle\n",
      "Bembidion | Bembidion\n",
      "Binomial nomenclature | Binomial Nomenclature\n",
      "California | California\n",
      "Doi (identifier) | Doi (Identifier)\n",
      "Global Biodiversity Information Facility | Global Biodiversity Information Facility\n",
      "Ground beetle | Ground Beetle\n",
      "Insect | Insect\n",
      "Mediterranean region | Mediterranean Region\n",
      "PMC (identifier) | Pmc (Identifier)\n",
      "PMID (identifier) | Pmid (Identifier)\n",
      "Pierre François Marie Auguste Dejean | Pierre François Marie Auguste Dejean\n",
      "Salt marsh | Salt Marsh\n",
      "San Francisco Bay | San Francisco Bay\n",
      "Taxonomy (biology) | Taxonomy (Biology)\n",
      "Trechinae | Trechinae\n",
      "Wikidata | Wikidata\n",
      "Wikispecies | Wikispecies\n"
     ]
    }
   ],
   "source": [
    "print(\"The Wikipedia page on '{}' has the title '{}' and contains {} links\".format(page,wiki.title,len(wiki.links)))\n",
    "for link in wiki.links:\n",
    "    print(link,'|',link.title())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our goal will be to analyze some notion which has a page in Wikipedia and to find important notions related to the given notion. We will do that by building and analyzing the so-called ego network around the given notion.\n",
    "\n",
    "At first, you should build the ego network, which is a subgraph of nodes that are close to the node representing the given notion. Then you will analyze the network.\n",
    "\n",
    "Your task is to implement two functions and then use them to analyze given notions.\n",
    "\n",
    "The first function \n",
    "```\n",
    "getWikipediaNetwork(ego,depth=2)\n",
    "```\n",
    "should build a **directed** network of nodes corresponding to the titles of all Wikipedia pages with distance at most `depth` from the starting page `ego`. The distance from page *A* to page *B* is the minimal number of links that should be clicked in order to get from page *A* to page *B*. **Remark:** Your network should contain all nodes of distance at most `depth` from `ego`, but you should collect all edges only for nodes with distance at most `(depth - 1)`.\n",
    "\n",
    "Your implementation should omit several types of pages that almost always create a dense structure close to virtually each Wikipedia page. You should omit at least all pages with the titles containing suffix \"(identifier)\" or \"(Identifier)\". See such pages in the above example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getWikipediaNetwork(ego,depth=2):\n",
    "    # create network - a directed graph of Wikipedia pages - around the \n",
    "    # page with title ego till the distance depth\n",
    "    #pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the function\n",
    "```\n",
    "getNetwork(name,depth=2,download=False)\n",
    "```\n",
    "\n",
    "that should create the ego-network (oriented network of type `nx.DiGraph()`)around page with title `name` of depth `depth`. If `download` is `True`, the network should be built by downloading pages from Wikipedia and the built network should be stored in the file with the name \n",
    "\n",
    "```\n",
    "name+'.csv'\n",
    "```\n",
    "If `download` is `False`, the function will check whether the file with the name `name+'.csv'` exists. If yes, it supposes that \n",
    "the file contains the edge list of the ego network around the notion from the parameter `name`\n",
    "and returns a network (`nx.DiGraph()`) with the network. Otherwise, it collects the network by calling\n",
    "the function `getWikipediaNetwork(name,depth)` and stores it in the corresponding CSV-file and returns the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def getNetwork(name,depth=2,download=False):\n",
    "    # create network around the Wikipedia page with title name. If download is False and a file\n",
    "    # <name>.csv exists, the network is read from the file, otherwise it is built using the function\n",
    "    # getWikipediaNetwork(name,depth)\n",
    "    #pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the above functions (and possibly other suitable functions) implement the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyzeEgo(G,ego,topNeighbours=25):\n",
    "    # analyze the ego network G with the root node ego\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "that takes the ego network `G` around node `ego`, deletes all nodes of degree at most one from the network and then finds `topNeighbours` nodes in the network with the highest **in-degree** except the `ego`. The function should  \n",
    "* print the number of nodes and edges of the original network,\n",
    "* print the number of nodes and edges of the network after truncation,\n",
    "* print value of at least two centrality measures for the `ego` node,\n",
    "* draw the subgraph of `G` on `topNeighbours` nodes with maximal in-degree and the `ego` node,\n",
    "and \n",
    "* return a list of pairs (node,in-degree) for `topNeighbours` nodes with maximal in-degree.\n",
    "\n",
    "After that, you should use your functions and analyze three ego networks:\n",
    "1. around `Network science`,\n",
    "2. around `Complex network` and \n",
    "3. around your personal favorite Wikipedia page.\n",
    "In each of the analyses, you should use and display 20 nodes around ego with the maximal in-degree.\n",
    "\n",
    "Are the networks comparable?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hints:**\n",
    "\n",
    "* The ego network around `Bembidion-ambiguum` has approximately 8185 nodes and 9763 edges,\n",
    "the degree of the node Bembidion-ambiguum is 17, its truncated network has 1210 nodes and 2788 edges\n",
    "* The ego network around `Complex network` has approximately 13830 nodes and 26037 edges,\n",
    "the degree of the node Complex network is 133, its truncated network has 3504 nodes and 15711 edges.\n",
    "\n",
    "Your values can differ slightly as the Wikipedia is changing and also some page can be unavailable.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
